{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T14:32:15.170395248Z",
     "start_time": "2023-09-15T14:32:14.645462270Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'experiments.log' # Normal local tests\n",
    "#filename = 'experiments_E.log' # manual virtual wall tests\n",
    "filename = 'experiments_0.log' # Virtual wall tests\n",
    "\n",
    "interval = 25\n",
    "batch_count = 12\n",
    "experiment_batches = [f'experiments_{i * interval}.log' for i in range(batch_count)]\n",
    "print(experiment_batches)\n",
    "\n",
    "selected_batch = 1\n",
    "filename = experiment_batches[selected_batch-1]\n",
    "print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T14:32:15.947966422Z",
     "start_time": "2023-09-15T14:32:15.170143155Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = tools.get_all_experiments(filename)\n",
    "# [experiment['directory'] for experiment in experiments]\n",
    "#len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the lowest experiment id, and remember the dir that belongs to it\n",
    "lowest_id = 1000000000000\n",
    "lowest_dir = 0\n",
    "for experiment in experiments:\n",
    "    if int(experiment['id']) < lowest_id:\n",
    "        lowest_id = int(experiment['id'])\n",
    "        lowest_dir = int(experiment['directory'])\n",
    "\n",
    "# dir is an integer as a string, we want to filter out all the experiments that have a lower dir\n",
    "experiments = [experiment for experiment in experiments if int(experiment['directory']) >= lowest_dir]\n",
    "\n",
    "len(experiments)\n",
    "\n",
    "#experiments[0]['dfs']['server_http']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_id_index(experiments):\n",
    "    if len(experiments) == 0:\n",
    "        return\n",
    "    # Search for the lowest id in the array: el['id]\n",
    "    min_id = int(experiments[0]['id'])\n",
    "    for experiment in experiments:\n",
    "        current_id = int(experiment['id'])\n",
    "        if current_id < min_id:\n",
    "            min_id = current_id\n",
    "    # Decrease the id of all the experiments by min_id\n",
    "    for experiment in experiments:\n",
    "        new_id = str(int(experiment['id']) - min_id)\n",
    "        experiment['id'] = new_id\n",
    "        experiment['info']['id'] = new_id\n",
    "\n",
    "reset_id_index(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T14:32:15.940215080Z"
    }
   },
   "outputs": [],
   "source": [
    "def take_window(df, column):\n",
    "    # Remove all the rows with 0 values, this removes the first rows\n",
    "    df = df.dropna().loc[(df!=0).any(axis=1)]\n",
    "    if (len(df) == 0):\n",
    "        return df\n",
    "    # Take the difference between rows, we want to find the last part where data is not increasing\n",
    "    df_diff = df.diff()\n",
    "    # Remove all non increasing rows\n",
    "    df_diff = df_diff.loc[(df_diff!=0).any(axis=1)]\n",
    "    if (len(df) == 0):\n",
    "        return df\n",
    "    # Get the last index of df_diff\n",
    "    last_index = df_diff.iloc[-1:].index[0]\n",
    "    # remove all rows from df after last_index\n",
    "    return df.loc[:last_index]\n",
    "\n",
    "def get_latency(experiments, should_round=False):\n",
    "    dfs_found = {}\n",
    "    events = ['multicast_files_sent', 'segments_requested']\n",
    "    output_events = [\"server_multicast multicast_files_sent\", \"client_1_1 segments_requested\"]\n",
    "    for experiment in experiments:\n",
    "        df = tools.get_metrics_by_events(\n",
    "            experiment['dfs'],\n",
    "            events,\n",
    "            interpolate=False,\n",
    "        )\n",
    "        if df.size > 0:            \n",
    "            has_all_columns = True\n",
    "            for event in output_events:\n",
    "                if event not in df.columns:\n",
    "                    has_all_columns = False\n",
    "                    break\n",
    "\n",
    "            if not has_all_columns:\n",
    "                continue\n",
    "\n",
    "            # Replace Nan, with the previous non Nan value\n",
    "            df = df.ffill()\n",
    "            \n",
    "            # Replace all NaN values with 0\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            # Floor all values\n",
    "            df = df.apply(np.floor)\n",
    "\n",
    "            df['in_transmission'] = df[output_events[0]] - df[output_events[1]]\n",
    "\n",
    "            intervals = []\n",
    "            in_transmission = False\n",
    "            start_time = None\n",
    "            end_time = None\n",
    "            for index, row in df.iterrows():\n",
    "                if row['in_transmission'] > 0:\n",
    "                    if not in_transmission:\n",
    "                        start_time = index\n",
    "                        in_transmission = True\n",
    "                else:\n",
    "                    if in_transmission:\n",
    "                        end_time = index\n",
    "                        in_transmission = False\n",
    "                        intervals.append((start_time, end_time))\n",
    "\n",
    "            interval_durations = [end_time - start_time for start_time, end_time in intervals]\n",
    "\n",
    "            # Get the experiment duration from the info dict\n",
    "            experiment_duration = int(experiment['info']['seg_dur'])\n",
    "            # Experiment duration is in seconds, convert to datetime\n",
    "            experiment_duration = datetime.timedelta(seconds=experiment_duration + 1) # Add 1 second to the experiment duration as a margin\n",
    "            # Remove interval durations that are longer than the experiment duration\n",
    "            interval_durations = [duration for duration in interval_durations if duration < experiment_duration]\n",
    "            \n",
    "            \n",
    "            avg_interval_duration = (sum(interval_durations, datetime.timedelta(0)) / len(interval_durations)) if len(interval_durations) > 0 else 0\n",
    "\n",
    "            result = {\n",
    "                'id': experiment['id'],\n",
    "                'event': 'total_latency',\n",
    "                'avg': avg_interval_duration,\n",
    "            }\n",
    "            dfs_found[experiment['id']] = result\n",
    "\n",
    "    return dfs_found\n",
    "\n",
    "\n",
    "def get_values(experiments, event, should_round=False):\n",
    "    dfs_found = {}\n",
    "    for experiment in experiments:\n",
    "        df = tools.get_metrics_by_events(\n",
    "            experiment['dfs'],\n",
    "            [event],\n",
    "            interpolate=False,\n",
    "        )\n",
    "        if df.size > 0:\n",
    "            df.columns = [experiment['id'] + ' ' + col for col in df.columns]\n",
    "            df_window = take_window(df, event)\n",
    "            if (len(df_window) == 0):\n",
    "                continue\n",
    "            # Get the first index of df_window\n",
    "            first_index = df_window.index[0]\n",
    "            # Get the last index of df_window\n",
    "            last_index = df_window.iloc[-1:].index[0]\n",
    "            # Get the first value\n",
    "            first_value = df_window.iloc[0].item()\n",
    "            # Get the last value\n",
    "            last_value = df_window.iloc[-1].item()\n",
    "            # Get the sum of all values\n",
    "            sum_value = df_window.sum().item()\n",
    "            # Get the max value\n",
    "            max_value = df_window.max().item()\n",
    "            # Get the difference between the first and last value\n",
    "            value_diff = (df_window.iloc[-1] - df_window.iloc[0]).item()\n",
    "            # Get the average value\n",
    "            avg_value = sum_value / len(df_window)\n",
    "\n",
    "            result = {\n",
    "                'id': experiment['id'],\n",
    "                'event': event,\n",
    "                'first': round(first_value, 3) if should_round else first_value,\n",
    "                'last': round(last_value, 3) if should_round else last_value,\n",
    "                'sum': round(sum_value, 3) if should_round else sum_value,\n",
    "                'max': round(max_value, 3) if should_round else max_value,\n",
    "                'diff': round(value_diff, 3) if should_round else value_diff,\n",
    "                'avg': round(avg_value, 3) if should_round else avg_value,\n",
    "                'start_index': first_index,\n",
    "                'end_index': last_index,\n",
    "            }\n",
    "            dfs_found[experiment['id']] = result\n",
    "\n",
    "    return dfs_found\n",
    "\n",
    "\n",
    "def get_MBps(experiments, event, start_end_times=None, time_window=None):\n",
    "    dfs_found = {}\n",
    "    for experiment in experiments:\n",
    "        df = tools.get_metrics_by_events(\n",
    "            experiment['dfs'],\n",
    "            [event],\n",
    "            interpolate=False,\n",
    "        )\n",
    "        if df.size > 0:\n",
    "            df.columns = [experiment['id'] + ' ' + col for col in df.columns]\n",
    "            df_window = take_window(df, event)\n",
    "            if (len(df_window) == 0):\n",
    "                continue\n",
    "\n",
    "            start_time = df_window.index[0]\n",
    "            end_time = df_window.index[-1]\n",
    "            \n",
    "            if start_end_times is not None:\n",
    "                times = start_end_times.get(experiment['id'])\n",
    "                if times is not None:\n",
    "                    start_time = times['start_time']\n",
    "                    end_time = times['end_time']\n",
    "            \n",
    "\n",
    "            # index is of type datetime, get the time difference between the first and last index\n",
    "            time_diff = time_window if time_window is not None and time_window > 0 else (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Get the difference between the first and last value\n",
    "            value_diff = (df_window.iloc[-1] - df_window.iloc[0]).item()\n",
    "            # Get MBps\n",
    "            MBps = value_diff / time_diff / 1000_000\n",
    "            # round to 2 decimals\n",
    "            MBps = round(MBps, 3)\n",
    "\n",
    "            result = {\n",
    "                'id': experiment['id'],\n",
    "                'event': event,\n",
    "                'time': time_diff,\n",
    "                'bytes': value_diff,\n",
    "                'Mbps': MBps * 8,\n",
    "            }\n",
    "            dfs_found[experiment['id']] = result\n",
    "\n",
    "    return dfs_found\n",
    "\n",
    "\n",
    "def convert_to_df(dictionaries, column_name, value_name='value'):\n",
    "    df = pd.DataFrame(dictionaries).T\n",
    "    if (len(df) == 0):\n",
    "        return df\n",
    "    df = df.pivot(index='id', columns='event', values=value_name).reset_index()\n",
    "    df = df.rename_axis(None, axis=1)\n",
    "    df = df.fillna(0)  # Replace NaN values with 0 if necessary\n",
    "    df['id'] = df['id'].astype(int)\n",
    "    df.set_index('id', inplace=True)  # Set 'id' as the index\n",
    "    # Get the event name from dict\n",
    "    event = list(dictionaries.values())[0]['event']\n",
    "\n",
    "    df = df.rename(columns={event: column_name})\n",
    "    return df.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_times_client = get_values(experiments, 'client_running')\n",
    "start_end_times_mc = get_values(experiments, 'is_multicasting')\n",
    "start_end_times = {}\n",
    "greatest_window = 0\n",
    "for time_client in start_end_times_client.values():\n",
    "    start_time = time_client['start_index']\n",
    "    end_time = time_client['end_index']\n",
    "\n",
    "    #print(f\"Experiment {time_client['id']}\")\n",
    "\n",
    "    #print(\"Client window: \", (end_time - start_time).total_seconds())\n",
    "\n",
    "    for time_mc in start_end_times_mc.values():\n",
    "        if time_mc['id'] == time_client['id']:\n",
    "            # If the start time is smaller than the client start time, use the client start time\n",
    "            if time_mc['start_index'] < time_client['start_index']:\n",
    "                start_time = time_mc['start_index']\n",
    "\n",
    "                #print(f\"Multicast window: \", (time_mc['end_index'] - time_mc['start_index']).total_seconds())\n",
    "            break\n",
    "\n",
    "    window = (end_time - start_time).total_seconds()\n",
    "\n",
    "    #print(f\"Window with multicast: {window}\")\n",
    "\n",
    "    greatest_window = max(greatest_window, window)\n",
    "    \n",
    "    start_end_times[time_client['id']] = {\n",
    "        'id': time_client['id'],\n",
    "        'start_time': start_time,\n",
    "        'end_time': end_time,\n",
    "    }\n",
    "\n",
    "print(f\"Greatest window: {greatest_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = greatest_window\n",
    "time_window = 96 # Total video length in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mc = get_MBps(experiments, 'total_bytes_mc_interface', time_window=time_window)\n",
    "dict_uc = get_MBps(experiments, 'total_bytes_uc_interface', time_window=time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = convert_to_df(dict_mc, 'MC Mbps', 'Mbps')\n",
    "df_uc = convert_to_df(dict_uc, 'UC Mbps', 'Mbps')\n",
    "\n",
    "merged_df = pd.concat([df_mc, df_uc], axis=1).sort_index(ascending=True)\n",
    "\n",
    "if False:\n",
    "    df_mc_bytes = convert_to_df(dict_mc, 'total MC [B]', 'bytes')\n",
    "    df_uc_bytes = convert_to_df(dict_uc, 'total UC [B]', 'bytes')\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df_mc_bytes, df_uc_bytes], axis=1).sort_index(ascending=True)\n",
    "\n",
    "if False:\n",
    "    df_mc_time = convert_to_df(dict_mc, 'total MC [s]', 'time')\n",
    "    df_uc_time = convert_to_df(dict_uc, 'total UC [s]', 'time')\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df_mc_time, df_uc_time], axis=1).sort_index(ascending=True)\n",
    "\n",
    "if False:\n",
    "    dict_mc_files = get_values(experiments, 'total_file_bytes_mc')\n",
    "    dict_uc_files = get_values(experiments, 'total_file_bytes_uc')\n",
    "    df_mc_files = convert_to_df(dict_mc_files, 'files MC [B]', 'last')\n",
    "    df_uc_files = convert_to_df(dict_uc_files, 'files UC [B]', 'last')\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df_mc_files, df_uc_files], axis=1).sort_index(ascending=True)\n",
    "\n",
    "if False:\n",
    "    dict_uc_fdt = get_values(experiments, 'total_fdt_bytes_uc')\n",
    "    dict_uc_partial = get_values(experiments, 'total_partial_bytes_uc')\n",
    "    df_uc_fdt = convert_to_df(dict_uc_fdt, 'fdt UC [B]', 'last')\n",
    "    df_uc_partial = convert_to_df(dict_uc_partial, 'partial UC [B]', 'last')\n",
    "\n",
    "if False:\n",
    "    dict_percentage_to_retrieve = get_values(experiments, 'alc_percentage_to_retrieve', should_round=True)\n",
    "    # Go over the dict and multiply the values by 100\n",
    "    for key, value in dict_percentage_to_retrieve.items():\n",
    "        value['avg'] = value['avg'] * 100\n",
    "    df_percentage_to_retrieve = convert_to_df(dict_percentage_to_retrieve, 'ALCs missing [%]', 'avg')\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df_percentage_to_retrieve], axis=1).sort_index(ascending=True)\n",
    "\n",
    "if False:\n",
    "    dict_files_sent = get_latency(experiments, should_round=True)\n",
    "    df_files_sent = convert_to_df(dict_files_sent, 'Total latency', 'avg')\n",
    "    print(df_files_sent)\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df_files_sent], axis=1).sort_index(ascending=True)\n",
    "\n",
    "def get_loss_from_experiments(experiments, experiment_id):\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == str(experiment_id):\n",
    "            return experiment['info']['loss']\n",
    "    return 0\n",
    "\n",
    "def get_timestamp_from_experiments(experiments, experiment_id):\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == str(experiment_id):\n",
    "            return experiment['info']['dir']\n",
    "    return 0\n",
    "\n",
    "def get_clients_from_experiments(experiments, experiment_id):\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == str(experiment_id):\n",
    "            return experiment['info']['n_clients']\n",
    "    return 0\n",
    "\n",
    "def get_proxies_from_experiments(experiments, experiment_id):\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == str(experiment_id):\n",
    "            return experiment['info']['n_subnets']\n",
    "    return 0\n",
    "\n",
    "if len(merged_df) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    odd_id_df = merged_df.loc[merged_df.index % 2 == 1]\n",
    "    even_id_df = merged_df.loc[merged_df.index % 2 == 0]\n",
    "\n",
    "    # Get the first row from even_id_df\n",
    "    first_row = even_id_df.iloc[0]\n",
    "    # Remove the first row from even_id_df\n",
    "    # even_id_df = even_id_df.iloc[1:]\n",
    "    # Add the row to the start of odd_id_df\n",
    "    odd_id_df = pd.concat([pd.DataFrame([first_row]), odd_id_df])\n",
    "\n",
    "\n",
    "\n",
    "    # Add column with losses to even_id_df\n",
    "    losses = odd_id_df.index.map(lambda experiment_id: get_loss_from_experiments(experiments, experiment_id))\n",
    "    timestamps = odd_id_df.index.map(lambda experiment_id: get_timestamp_from_experiments(experiments, experiment_id))\n",
    "    clients = odd_id_df.index.map(lambda experiment_id: get_clients_from_experiments(experiments, experiment_id))\n",
    "    proxies = odd_id_df.index.map(lambda experiment_id: get_proxies_from_experiments(experiments, experiment_id))\n",
    "\n",
    "    # Rename the columns in the even_id_df to append 'FEC' to their names\n",
    "    even_id_df.columns = [f'{col} (FEC)' for col in even_id_df.columns]\n",
    "    # Decrease the index of even_id_df by 1\n",
    "    even_id_df.index = even_id_df.index - 1\n",
    "    # Set the row with index -1 to index 0\n",
    "    even_id_df = even_id_df.rename(index={-1: 0})\n",
    "\n",
    "    # Combine the two DataFrames side by side\n",
    "    merged_df = pd.concat([odd_id_df, even_id_df], axis=1).sort_index(ascending=True)\n",
    "    # Check if length of losses is the same, if not then add '?' untill it is equal length to losses\n",
    "    while len(losses) < len(merged_df):\n",
    "        losses = np.append(losses, '?')\n",
    "    merged_df.insert(0, 'MC loss %', losses)\n",
    "    while len(timestamps) < len(merged_df):\n",
    "        timestamps = np.append(timestamps, '?')\n",
    "    merged_df.insert(0, 'timestamp', timestamps)\n",
    "    while len(clients) < len(merged_df):\n",
    "        clients = np.append(clients, '?')\n",
    "    merged_df.insert(0, 'CPP', clients)\n",
    "    while len(proxies) < len(merged_df):\n",
    "        proxies = np.append(proxies, '?')\n",
    "    merged_df.insert(0, 'P', proxies)\n",
    "\n",
    "    # All the experiments are multicast, except for the first one\n",
    "    merged_df.insert(0, 'MC', 1)\n",
    "    merged_df.iloc[0, 0] = 0\n",
    "    # Set the value of 'MC loss %' to 100 for the first row\n",
    "    merged_df.iloc[0, 4] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[0]['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged_df to csv, ask for filename first\n",
    "filename = input(\"Filename: \")\n",
    "# If input was not cancelled, save the file\n",
    "if filename != '':\n",
    "    # Replace nan first with 0\n",
    "    temp_df = merged_df.fillna(0)\n",
    "    temp_df.to_csv(f'./{filename}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "from scipy import interpolate\n",
    "\n",
    "def plot(data, y_axis='Mbps'):\n",
    "    if data.empty:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(30,7))\n",
    "    start_time = float(data.index[0])\n",
    "    end_time = float(data.index[-1]) if not isinstance(data.index[-1], str) else start_time + 1\n",
    "\n",
    "    for column in data.columns:\n",
    "        x_data = data.index.to_numpy()\n",
    "        y_data = data[column].to_numpy()\n",
    "        x_data = x_data.astype(float)\n",
    "        y_data = y_data.astype(float)\n",
    "        flinear = interpolate.interp1d(x_data, y_data)\n",
    "        x_new = np.linspace(int(min(x_data)), int(max(x_data)), int(max(x_data))+1)\n",
    "        y_new = flinear(x_new)\n",
    "        \n",
    "        latest_color = plt.gca()._get_lines.get_next_color()\n",
    "        ax.plot(x_new, y_new, alpha=0.5, color=latest_color, label=column, linewidth=2.5)\n",
    "        ax.scatter(x_data, y_data, marker='o', color=latest_color, linewidth=2.5)\n",
    "\n",
    "    ax.set_xlim(start_time, end_time)  # Set the x-axis limits based on data range\n",
    "    ax.set_title('Bandwidth')\n",
    "    ax.set_xlabel('Loss')\n",
    "    ax.set_ylabel(y_axis)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if len(merged_df) > 0:\n",
    "    df_copy = merged_df[['MC loss %', 'MC Mbps', 'UC Mbps','MC Mbps (FEC)','UC Mbps (FEC)']].copy().iloc[1:]\n",
    "    df_copy.set_index('MC loss %', inplace=True)\n",
    "    df_copy.columns = df_copy.columns.str.replace(r' Mbps', '')\n",
    "    # replace nan with 0\n",
    "    df_copy = df_copy.fillna(0)\n",
    "\n",
    "    plot(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['MC FEC overhead'] = ((df_copy['MC (FEC)'] - df_copy['MC']) / df_copy['MC']) * 100\n",
    "df_copy['UC FEC overhead'] = ((df_copy['UC (FEC)'] - df_copy['UC']) / df_copy['UC']) * 100\n",
    "\n",
    "plot(df_copy[['MC FEC overhead', 'UC FEC overhead']], y_axis='%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[['MC FEC overhead', 'UC FEC overhead']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
